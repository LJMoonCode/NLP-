{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RRFtIHFscrmS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, dim_num = 512, head_num = 8):\n",
        "    super().__init__()\n",
        "    self.head_num = head_num\n",
        "    self.dim_num = dim_num\n",
        "\n",
        "    #레이어 할당\n",
        "    self.fc_q = nn.Linear(dim_num, dim_num)\n",
        "    self.fc_k = nn.Linear(dim_num, dim_num)\n",
        "    self.fc_v = nn.Linear(dim_num, dim_num)\n",
        "\n",
        "    self.fc_o = nn.Linear(dim_num, dim_num)\n",
        "\n",
        "  def ScaledDotProductAttention(self, q, k, v, mask = None):\n",
        "    dk = k.shape[-1]\n",
        "    kt = k.permute(0, 2, 1, 3)\n",
        "\n",
        "    output = torch.matmul(q, kt)\n",
        "    output = output / math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "      output = output.masked_fill(mask.unsqueeze(1).unsqueeze(-1), 0)\n",
        "\n",
        "    output = nn.softmax(output, -1)\n",
        "    output = torch.matmul(output, v)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def forward(self, q, k, v, mask = None):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    q = self.fc_q(q).view(batch_size, -1, self.head_num, self.dim_num//self.head_num).permute(0, 2, 1, 3)\n",
        "    k = self.fc_k(k).view(batch_size, -1, self.head_num, self.dim_num//self.head_num).permute(0, 2, 1, 3)\n",
        "    v = self.fc_v(v).view(batch_size, -1, self.head_num, self.dim_num//self.head_num).permute(0, 2, 1, 3)\n",
        "\n",
        "    output = self.ScaledDotProductAttention(q, k, v, mask)\n",
        "    batch_num, head_num, seq_num, hidden_num = output.size()\n",
        "    output = output.permute(0, 2, 1, 3).contiguous()\n",
        "    output = output.view(batch_size, -1, self.hidden_dim)\n",
        "    output = self.fc_o(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "otV_v5TRdh2I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddLayerNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def layer_norm(self, input):\n",
        "    mean = torch.mean(input, dim = -1, keepdim= True)\n",
        "    std = torch.std(input, dim = -1, keepdim= True)\n",
        "    output = (input - mean) / std\n",
        "    return output\n",
        "\n",
        "  def forward(self, input, residual):\n",
        "    return residual + self.layer_norm(input)"
      ],
      "metadata": {
        "id": "LeqQPejhqqw1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dim_num = 512):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(dim_num, dim_num * 4)\n",
        "    self.layer2 = nn.Linear(dim_num * 4, dim_num)\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.layer1(input)\n",
        "    output = self.layer2(nn.ReLU(output))\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "80wcNLmSrPw0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, dim_num = 512):\n",
        "    super().__init__()\n",
        "    self.multihead = MultiHeadAttention(dim_num=dim_num)\n",
        "    self.addnorm1 = AddLayerNorm()\n",
        "    self.feedforward = FeedForward(dim_num=dim_num)\n",
        "    self.addnorm2 = AddLayerNorm()\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    multihead_output = self.multihead(q, k, v)\n",
        "    addnorm1_output = self.addnorm1(multihead_output, q)\n",
        "    feedforward_output = self.feedforward(addnorm1_output)\n",
        "    addnorm2_output = self.addnorm2(feedforward_output, addnorm1_output)\n",
        "\n",
        "    return addnorm2_output\n"
      ],
      "metadata": {
        "id": "fIoat0xary_I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, dim_num = 512):\n",
        "    super().__init__()\n",
        "    self.masked_multihead = MultiHeadAttention(dim_num=dim_num)\n",
        "    self.addnorm1 = AddLayerNorm()\n",
        "    self.multihead = MultiHeadAttention(dim_num=dim_num)\n",
        "    self.addnorm2 = AddLayerNorm()\n",
        "    self.feedforward = FeedForward(dim_num=dim_num)\n",
        "    self.addnorm3 = AddLayerNorm()\n",
        "\n",
        "  def forward(self, q, k, v, encoder_output, mask):\n",
        "    masked_multihead_output = self.masked_multihead(q, k, v, mask)\n",
        "    addnorm1_output = self.addnorm1(masked_multihead_output, q)\n",
        "    #여기서 왜 encoder output이 두개로 할당돼서 들어가는거지..?\n",
        "    multihead_output = self.multihead(encoder_output, encoder_output, addnorm1_output, mask)\n",
        "    addnorm2_output = self.addnorm2(multihead_output, addnorm1_output)\n",
        "    feedforward_output = self.feed_forward(addnorm2_output)\n",
        "    addnorm3_output = self.addnorm3(feedforward_output, addnorm2_output)\n",
        "\n",
        "    return addnorm3_output"
      ],
      "metadata": {
        "id": "vSLAeNUVtAId"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder_num = 6, decoder_num = 6, hidden_dim = 512, max_encoder_seq_length = 100, max_decoder_seq_length = 100):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_num = encoder_num\n",
        "    self.decoder_num = decoder_num\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.max_encoder_seq_length = max_encoder_seq_length\n",
        "    self.max_decoder_seq_length = max_decoder_seq_length\n",
        "\n",
        "    self.input_data_embeded = nn.Embedding(max_encoder_seq_length, self.hidden_dim)\n",
        "    self.Encoders = [Encoder(dim_num = hidden_dim) for _ in range(encoder_num)]\n",
        "\n",
        "    self.output_data_embeded = nn.Embedding(max_decoder_seq_length, self.hidden_dim)\n",
        "    self.Decoders = [Decoder(dim_num = hidden_dim) for _ in range(decoder_num)]\n",
        "\n",
        "    self.last_linear_layer = nn.Linear(self.hidden_dim, max_decoder_seq_length)\n",
        "\n",
        "  #이쪽이 이해가 잘 안 되네..\n",
        "  def position_encoding(self, position_max_length = 100):\n",
        "    position = torch.arange(0, position_max_length, dytpe = torch.float).unsqueeze(1)\n",
        "    pe = torch.zeros(position_max_length, self.hidden_dim)\n",
        "    div_term = torch.pow(torch.ones(self.hidden_dim // 2).fill_(10000),\n",
        "                         torch.arange(0, self.hidden_dim, 2) / torch.tensor(self.hidden_dim, dtype = torch.float32))\n",
        "\n",
        "    pe[:,0::2] = torch.sin(position / div_term)\n",
        "    pe[:,1::2] = torch.cos(position / div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "    return pe\n",
        "\n",
        "  def forward(self, input, output, mask):\n",
        "    input_embeded = self.input_data_embeded(input)\n",
        "    input_embeded += self.position_encoding(self.max_encoder_seq_length)\n",
        "    q, k, v = input_embeded, input_embeded, input_embeded\n",
        "\n",
        "    for encoder in self.Encoders:\n",
        "      encoder_output = encoder(q, k, v)\n",
        "      q, k, v = encoder_output, encoder_output, encoder_output\n",
        "\n",
        "    output_embeded = self.output_data_embeded(output)\n",
        "    output_embeded += self.position_encoding(self.max_decoder_seq_length)\n",
        "    output_embeded = output_embeded.masked_fill(mask.unsqueeze(-1), 0)\n",
        "    d_q, d_k, d_v = output_embeded, output_embeded, output_embeded\n",
        "\n",
        "    for decoder in self.Decoders:\n",
        "      decoder_output = decoder(d_q, d_k, d_v, encoder_output, mask)\n",
        "      d_q, d_k, d_v = decoder_output, decoder_output, decoder_output\n",
        "\n",
        "    output = nn.Softmax(self.last_linear_layer(decoder_output), dim = -1)\n",
        "    return output\n",
        ""
      ],
      "metadata": {
        "id": "9gmBRRplQ0uH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unfG7l_TTq7c",
        "outputId": "ff4270b1-0cfb-49bf-cce4-7ea38342c529"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/64.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "Successfully installed torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "metadata": {
        "id": "AkPiqUFUWhk5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "spacy_de = spacy.load(\"de_core_news_sm\")"
      ],
      "metadata": {
        "id": "UNsp9fAhWy5C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = spacy_en.tokenizer('I am a graduate student.')\n",
        "\n",
        "for i, token in enumerate(tokenized):\n",
        "  print(f'인덱스 {i}: {token.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmdfdELKXGI7",
        "outputId": "5e2273d2-3c03-4c9a-fa3f-6ff3ebf392f4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: am\n",
            "인덱스 2: a\n",
            "인덱스 3: graduate\n",
            "인덱스 4: student\n",
            "인덱스 5: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_de(text):\n",
        "  return [token.text for token in spacy_de.tokenizer(text)]\n",
        "def tokenize_en(text):\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]\n"
      ],
      "metadata": {
        "id": "TL_reUg2XtDT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "SRC = Field(tokenize = tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\n",
        "TRG = Field(tokenize = tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)"
      ],
      "metadata": {
        "id": "APYUnUhHX6Fl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG), root = 'data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "tJAQfvEIYUt4",
        "outputId": "21818ddd-fb25-4eb3-f142-7e6962d91022"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/multi30k/train.de'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-18e190c364c9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMulti30k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulti30k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".de\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/datasets/translation.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, exts, fields, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         return super(Multi30k, cls).splits(\n\u001b[0m\u001b[1;32m    114\u001b[0m             exts, fields, path, root, train, validation, test, **kwargs)\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/datasets/translation.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, exts, fields, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         train_data = None if train is None else cls(\n\u001b[0m\u001b[1;32m     66\u001b[0m             os.path.join(path, train), exts, fields, **kwargs)\n\u001b[1;32m     67\u001b[0m         val_data = None if validation is None else cls(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/datasets/translation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, exts, fields, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrg_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msrc_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/multi30k/train.de'"
          ]
        }
      ]
    }
  ]
}